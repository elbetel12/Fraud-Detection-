{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e1fc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PART 1: E-COMMERCE FRAUD DETECTION MODELING\n",
      "============================================================\n",
      "\n",
      "1. Loading feature-engineered e-commerce data...\n",
      "Dataset shape: (151112, 50)\n",
      "Columns: 50\n",
      "Fraud rate: 9.36%\n",
      "\n",
      "Missing values in dataset: 151112\n",
      "\n",
      "Columns with missing values:\n",
      "  user_std_amount: 151112 missing (100.00%)\n",
      "\n",
      "2. Preparing e-commerce data for modeling...\n",
      "Feature matrix shape: (151112, 49)\n",
      "Target shape: (151112,)\n",
      "\n",
      "3. Handling missing values with simple approach...\n",
      "  Dropping column 'user_std_amount' - 100.0% missing\n",
      "\n",
      "After cleaning: (151112, 48)\n",
      "Remaining missing values: 0\n",
      "\n",
      "4. Converting boolean columns to integer...\n",
      "Converting 4 boolean columns to integer\n",
      "Final feature matrix shape: (151112, 48)\n",
      "\n",
      "5. Creating train-test split (80-20 stratified)...\n",
      "Training set: 120,889 samples\n",
      "Test set: 30,223 samples\n",
      "Training fraud rate: 9.36%\n",
      "Test fraud rate: 9.36%\n",
      "\n",
      "6. Applying SMOTE to handle class imbalance...\n",
      "NaN values in training features: 0\n",
      "NaN values in training target: 0\n",
      "Before SMOTE - Training shape: (120889, 48)\n",
      "After SMOTE - Training shape: (219136, 48)\n",
      "Before SMOTE - Fraud rate: 9.36%\n",
      "After SMOTE - Fraud rate: 50.00%\n",
      "SMOTE object saved to '../models/ecommerce_smote.pkl'\n",
      "\n",
      "7. Scaling features...\n",
      "Scaler saved to '../models/ecommerce_scaler.pkl'\n",
      "\n",
      "============================================================\n",
      "MODEL 1: LOGISTIC REGRESSION (Baseline)\n",
      "============================================================\n",
      "\n",
      "Logistic Regression Performance:\n",
      "  Training_Time: 8.7300\n",
      "  Accuracy: 0.9873\n",
      "  Precision: 0.9058\n",
      "  Recall: 0.9650\n",
      "  F1_Score: 0.9345\n",
      "  ROC_AUC: 0.9990\n",
      "  PR_AUC: 0.9909\n",
      "  True_Negative: 27109\n",
      "  False_Positive: 284\n",
      "  False_Negative: 99\n",
      "  True_Positive: 2731\n",
      "  False_Positive_Rate: 0.0104\n",
      "  False_Negative_Rate: 0.0350\n",
      "  Business_Cost_Score: 0.4215\n",
      "\n",
      "============================================================\n",
      "MODEL 2: RANDOM FOREST\n",
      "============================================================\n",
      "\n",
      "Random Forest Performance:\n",
      "  Training_Time: 22.5200\n",
      "  Accuracy: 0.9872\n",
      "  Precision: 0.9216\n",
      "  Recall: 0.9435\n",
      "  F1_Score: 0.9324\n",
      "  ROC_AUC: 0.9990\n",
      "  PR_AUC: 0.9910\n",
      "  True_Negative: 27166\n",
      "  False_Positive: 227\n",
      "  False_Negative: 160\n",
      "  True_Positive: 2670\n",
      "  False_Positive_Rate: 0.0083\n",
      "  False_Negative_Rate: 0.0565\n",
      "  Business_Cost_Score: 0.6045\n",
      "\n",
      "============================================================\n",
      "MODEL 3: XGBOOST\n",
      "============================================================\n",
      "\n",
      "XGBoost Performance:\n",
      "  Training_Time: 37.5100\n",
      "  Accuracy: 0.9861\n",
      "  Precision: 0.8726\n",
      "  Recall: 0.9975\n",
      "  F1_Score: 0.9309\n",
      "  ROC_AUC: 0.9991\n",
      "  PR_AUC: 0.9918\n",
      "  True_Negative: 26981\n",
      "  False_Positive: 412\n",
      "  False_Negative: 7\n",
      "  True_Positive: 2823\n",
      "  False_Positive_Rate: 0.0150\n",
      "  False_Negative_Rate: 0.0025\n",
      "  Business_Cost_Score: 0.1595\n",
      "\n",
      "============================================================\n",
      "MODEL 4: LIGHTGBM\n",
      "============================================================\n",
      "\n",
      "LightGBM Performance:\n",
      "  Training_Time: 4.8200\n",
      "  Accuracy: 0.9878\n",
      "  Precision: 0.9253\n",
      "  Recall: 0.9456\n",
      "  F1_Score: 0.9353\n",
      "  ROC_AUC: 0.9991\n",
      "  PR_AUC: 0.9915\n",
      "  True_Negative: 27177\n",
      "  False_Positive: 216\n",
      "  False_Negative: 154\n",
      "  True_Positive: 2676\n",
      "  False_Positive_Rate: 0.0079\n",
      "  False_Negative_Rate: 0.0544\n",
      "  Business_Cost_Score: 0.5810\n",
      "\n",
      "============================================================\n",
      "E-COMMERCE MODEL COMPARISON\n",
      "============================================================\n",
      "\n",
      "Performance Comparison Table:\n",
      "              Model  Accuracy  Precision  Recall  F1_Score  ROC_AUC  PR_AUC  False_Positive_Rate  False_Negative_Rate  Business_Cost_Score\n",
      "Logistic Regression    0.9873     0.9058  0.9650    0.9345   0.9990  0.9909               0.0104               0.0350               0.4215\n",
      "      Random Forest    0.9872     0.9216  0.9435    0.9324   0.9990  0.9910               0.0083               0.0565               0.6045\n",
      "            XGBoost    0.9861     0.8726  0.9975    0.9309   0.9991  0.9918               0.0150               0.0025               0.1595\n",
      "           LightGBM    0.9878     0.9253  0.9456    0.9353   0.9991  0.9915               0.0079               0.0544               0.5810\n",
      "\n",
      "Model comparison saved to '../data/processed/ecommerce_model_comparison.csv'\n",
      "\n",
      "============================================================\n",
      "CROSS-VALIDATION FOR BEST E-COMMERCE MODEL\n",
      "============================================================\n",
      "Best model based on PR-AUC: XGBoost\n",
      "\n",
      "Performing 5-fold cross-validation for XGBoost...\n",
      "Cross-validation PR-AUC scores: [0.99990972 0.99989164 0.99991211 0.99988513 0.99991266]\n",
      "Mean PR-AUC: 0.9999 (+/- 0.0000)\n",
      "\n",
      "Saving best e-commerce model: XGBoost\n",
      "Model saved to '../models/best_ecommerce_model_XGBoost.pkl'\n",
      "Feature names saved to '../models/ecommerce_feature_names.pkl'\n",
      "\n",
      "============================================================\n",
      "E-COMMERCE MODELING COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Best model: XGBoost\n",
      "PR-AUC: 0.9918\n",
      "Recall: 0.9975\n",
      "False Positive Rate: 0.0150\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Task 2: Model Building and Training for Fraud Detection\n",
    "# \n",
    "# **Objective**: Build, train, and evaluate classification models to detect fraudulent transactions, \n",
    "# using appropriate techniques for imbalanced data.\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, \n",
    "                           roc_auc_score, average_precision_score, \n",
    "                           precision_recall_curve, roc_curve, f1_score,\n",
    "                           precision_score, recall_score, accuracy_score)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# %%\n",
    "# ============================================\n",
    "# PART 1: E-COMMERCE FRAUD MODELING\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PART 1: E-COMMERCE FRAUD DETECTION MODELING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# %%\n",
    "# Load feature-engineered e-commerce data\n",
    "print(\"\\n1. Loading feature-engineered e-commerce data...\")\n",
    "ecom_data = pd.read_csv('../data/processed/fraud_data_featured.csv')\n",
    "print(f\"Dataset shape: {ecom_data.shape}\")\n",
    "print(f\"Columns: {len(ecom_data.columns)}\")\n",
    "print(f\"Fraud rate: {ecom_data['class'].mean()*100:.2f}%\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values in dataset: {ecom_data.isnull().sum().sum()}\")\n",
    "if ecom_data.isnull().sum().sum() > 0:\n",
    "    print(\"\\nColumns with missing values:\")\n",
    "    missing_info = ecom_data.isnull().sum()\n",
    "    missing_info = missing_info[missing_info > 0]\n",
    "    for col, count in missing_info.items():\n",
    "        missing_pct = count / len(ecom_data) * 100\n",
    "        print(f\"  {col}: {count} missing ({missing_pct:.2f}%)\")\n",
    "\n",
    "# %%\n",
    "# Data preparation for e-commerce - SIMPLIFIED APPROACH\n",
    "print(\"\\n2. Preparing e-commerce data for modeling...\")\n",
    "\n",
    "# Separate features and target\n",
    "X_ecom = ecom_data.drop('class', axis=1)\n",
    "y_ecom = ecom_data['class']\n",
    "\n",
    "print(f\"Feature matrix shape: {X_ecom.shape}\")\n",
    "print(f\"Target shape: {y_ecom.shape}\")\n",
    "\n",
    "# SIMPLE MISSING VALUE HANDLING\n",
    "print(\"\\n3. Handling missing values with simple approach...\")\n",
    "\n",
    "# Strategy: Remove columns with more than 50% missing values, fill others\n",
    "missing_threshold = 0.5  # 50% threshold\n",
    "\n",
    "columns_to_keep = []\n",
    "for col in X_ecom.columns:\n",
    "    missing_pct = X_ecom[col].isnull().sum() / len(X_ecom)\n",
    "    if missing_pct < missing_threshold:\n",
    "        columns_to_keep.append(col)\n",
    "    else:\n",
    "        print(f\"  Dropping column '{col}' - {missing_pct*100:.1f}% missing\")\n",
    "\n",
    "X_ecom = X_ecom[columns_to_keep]\n",
    "\n",
    "# Fill remaining missing values with column median\n",
    "for col in X_ecom.columns:\n",
    "    if X_ecom[col].isnull().any():\n",
    "        X_ecom[col] = X_ecom[col].fillna(X_ecom[col].median())\n",
    "\n",
    "print(f\"\\nAfter cleaning: {X_ecom.shape}\")\n",
    "print(f\"Remaining missing values: {X_ecom.isnull().sum().sum()}\")\n",
    "\n",
    "# %%\n",
    "# Convert boolean columns to integer\n",
    "print(\"\\n4. Converting boolean columns to integer...\")\n",
    "bool_cols = X_ecom.select_dtypes(include=['bool']).columns.tolist()\n",
    "if bool_cols:\n",
    "    print(f\"Converting {len(bool_cols)} boolean columns to integer\")\n",
    "    X_ecom[bool_cols] = X_ecom[bool_cols].astype(int)\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "for col in X_ecom.columns:\n",
    "    if X_ecom[col].dtype == 'object':\n",
    "        print(f\"  Converting object column '{col}' to numeric...\")\n",
    "        X_ecom[col] = pd.to_numeric(X_ecom[col], errors='coerce')\n",
    "        X_ecom[col] = X_ecom[col].fillna(X_ecom[col].median())\n",
    "\n",
    "print(f\"Final feature matrix shape: {X_ecom.shape}\")\n",
    "\n",
    "# %%\n",
    "# Train-test split with stratification\n",
    "print(\"\\n5. Creating train-test split (80-20 stratified)...\")\n",
    "X_train_ecom, X_test_ecom, y_train_ecom, y_test_ecom = train_test_split(\n",
    "    X_ecom, y_ecom, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_ecom\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_ecom.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test_ecom.shape[0]:,} samples\")\n",
    "print(f\"Training fraud rate: {y_train_ecom.mean()*100:.2f}%\")\n",
    "print(f\"Test fraud rate: {y_test_ecom.mean()*100:.2f}%\")\n",
    "\n",
    "# %%\n",
    "# Handle class imbalance using SMOTE (for e-commerce data)\n",
    "print(\"\\n6. Applying SMOTE to handle class imbalance...\")\n",
    "\n",
    "# Final check for NaN values\n",
    "print(f\"NaN values in training features: {np.isnan(X_train_ecom.values).sum()}\")\n",
    "print(f\"NaN values in training target: {y_train_ecom.isnull().sum()}\")\n",
    "\n",
    "# Ensure no NaN values remain\n",
    "if np.isnan(X_train_ecom.values).sum() > 0:\n",
    "    print(\"Filling remaining NaN values...\")\n",
    "    X_train_ecom = X_train_ecom.fillna(X_train_ecom.median())\n",
    "\n",
    "# Apply SMOTE only to training data\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_ecom, y_train_ecom)\n",
    "\n",
    "print(f\"Before SMOTE - Training shape: {X_train_ecom.shape}\")\n",
    "print(f\"After SMOTE - Training shape: {X_train_smote.shape}\")\n",
    "print(f\"Before SMOTE - Fraud rate: {y_train_ecom.mean()*100:.2f}%\")\n",
    "print(f\"After SMOTE - Fraud rate: {y_train_smote.mean()*100:.2f}%\")\n",
    "\n",
    "# Save the SMOTE object for later use\n",
    "joblib.dump(smote, '../models/ecommerce_smote.pkl')\n",
    "print(\"SMOTE object saved to '../models/ecommerce_smote.pkl'\")\n",
    "\n",
    "# %%\n",
    "# Scale features (important for Logistic Regression)\n",
    "print(\"\\n7. Scaling features...\")\n",
    "scaler_ecom = StandardScaler()\n",
    "X_train_scaled = scaler_ecom.fit_transform(X_train_smote)\n",
    "X_test_scaled = scaler_ecom.transform(X_test_ecom)\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler_ecom, '../models/ecommerce_scaler.pkl')\n",
    "print(\"Scaler saved to '../models/ecommerce_scaler.pkl'\")\n",
    "\n",
    "# %%\n",
    "# Define evaluation function\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation function\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Training_Time': round(training_time, 2),\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1_Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'ROC_AUC': roc_auc_score(y_test, y_pred_proba),\n",
    "        'PR_AUC': average_precision_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    # Additional metrics for business context\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    metrics.update({\n",
    "        'True_Negative': tn,\n",
    "        'False_Positive': fp,\n",
    "        'False_Negative': fn,\n",
    "        'True_Positive': tp,\n",
    "        'False_Positive_Rate': fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        'False_Negative_Rate': fn / (fn + tp) if (fn + tp) > 0 else 0,\n",
    "        'Business_Cost_Score': (fn * 100 + fp * 10) / len(y_test)  # Simplified cost model\n",
    "    })\n",
    "    \n",
    "    return metrics, model, y_pred, y_pred_proba, cm\n",
    "\n",
    "# %%\n",
    "# Baseline Model 1: Logistic Regression\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 1: LOGISTIC REGRESSION (Baseline)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    solver='liblinear',\n",
    "    C=1.0\n",
    ")\n",
    "\n",
    "lr_metrics, lr_model_fitted, y_pred_lr, y_pred_proba_lr, cm_lr = evaluate_model(\n",
    "    lr_model, X_train_scaled, X_test_scaled, y_train_smote, y_test_ecom, \n",
    "    \"Logistic Regression\"\n",
    ")\n",
    "\n",
    "print(\"\\nLogistic Regression Performance:\")\n",
    "for key, value in lr_metrics.items():\n",
    "    if key != 'Model':\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "# %%\n",
    "# Model 2: Random Forest\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 2: RANDOM FOREST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_metrics, rf_model_fitted, y_pred_rf, y_pred_proba_rf, cm_rf = evaluate_model(\n",
    "    rf_model, X_train_smote, X_test_ecom, y_train_smote, y_test_ecom, \n",
    "    \"Random Forest\"\n",
    ")\n",
    "\n",
    "print(\"\\nRandom Forest Performance:\")\n",
    "for key, value in rf_metrics.items():\n",
    "    if key != 'Model':\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "# %%\n",
    "# Model 3: XGBoost\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 3: XGBOOST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate scale_pos_weight for imbalance\n",
    "scale_pos_weight = len(y_train_ecom[y_train_ecom==0]) / len(y_train_ecom[y_train_ecom==1])\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_metrics, xgb_model_fitted, y_pred_xgb, y_pred_proba_xgb, cm_xgb = evaluate_model(\n",
    "    xgb_model, X_train_smote, X_test_ecom, y_train_smote, y_test_ecom, \n",
    "    \"XGBoost\"\n",
    ")\n",
    "\n",
    "print(\"\\nXGBoost Performance:\")\n",
    "for key, value in xgb_metrics.items():\n",
    "    if key != 'Model':\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "# %%\n",
    "# Model 4: LightGBM\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 4: LIGHTGBM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lgbm_model = LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    verbose=-1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm_metrics, lgbm_model_fitted, y_pred_lgbm, y_pred_proba_lgbm, cm_lgbm = evaluate_model(\n",
    "    lgbm_model, X_train_smote, X_test_ecom, y_train_smote, y_test_ecom, \n",
    "    \"LightGBM\"\n",
    ")\n",
    "\n",
    "print(\"\\nLightGBM Performance:\")\n",
    "for key, value in lgbm_metrics.items():\n",
    "    if key != 'Model':\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "# %%\n",
    "# Compare all e-commerce models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"E-COMMERCE MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Collect all metrics\n",
    "ecom_metrics_list = [lr_metrics, rf_metrics, xgb_metrics, lgbm_metrics]\n",
    "ecom_metrics_df = pd.DataFrame(ecom_metrics_list)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(\"\\nPerformance Comparison Table:\")\n",
    "comparison_cols = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1_Score', 'ROC_AUC', 'PR_AUC', \n",
    "                   'False_Positive_Rate', 'False_Negative_Rate', 'Business_Cost_Score']\n",
    "print(ecom_metrics_df[comparison_cols].to_string(index=False))\n",
    "\n",
    "# %%\n",
    "# Save e-commerce model comparison\n",
    "ecom_metrics_df.to_csv('../data/processed/ecommerce_model_comparison.csv', index=False)\n",
    "print(\"\\nModel comparison saved to '../data/processed/ecommerce_model_comparison.csv'\")\n",
    "\n",
    "# %%\n",
    "# Cross-validation for the best model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CROSS-VALIDATION FOR BEST E-COMMERCE MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select best model based on PR-AUC\n",
    "best_ecom_model_name = ecom_metrics_df.loc[ecom_metrics_df['PR_AUC'].idxmax(), 'Model']\n",
    "print(f\"Best model based on PR-AUC: {best_ecom_model_name}\")\n",
    "\n",
    "# Get the best model\n",
    "if best_ecom_model_name == \"Logistic Regression\":\n",
    "    best_ecom_model = lr_model_fitted\n",
    "    X_cv = X_train_scaled\n",
    "elif best_ecom_model_name == \"Random Forest\":\n",
    "    best_ecom_model = rf_model_fitted\n",
    "    X_cv = X_train_smote\n",
    "elif best_ecom_model_name == \"XGBoost\":\n",
    "    best_ecom_model = xgb_model_fitted\n",
    "    X_cv = X_train_smote\n",
    "else:\n",
    "    best_ecom_model = lgbm_model_fitted\n",
    "    X_cv = X_train_smote\n",
    "\n",
    "# Perform Stratified K-Fold Cross Validation\n",
    "print(f\"\\nPerforming 5-fold cross-validation for {best_ecom_model_name}...\")\n",
    "\n",
    "# Use the training data for CV\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(\n",
    "    best_ecom_model, X_cv, y_train_smote, \n",
    "    cv=skf, \n",
    "    scoring='average_precision',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"Cross-validation PR-AUC scores: {cv_scores}\")\n",
    "print(f\"Mean PR-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "\n",
    "# %%\n",
    "# Save the best e-commerce model\n",
    "print(f\"\\nSaving best e-commerce model: {best_ecom_model_name}\")\n",
    "model_filename = f'../models/best_ecommerce_model_{best_ecom_model_name.replace(\" \", \"_\")}.pkl'\n",
    "joblib.dump(best_ecom_model, model_filename)\n",
    "print(f\"Model saved to '{model_filename}'\")\n",
    "\n",
    "# Save feature names\n",
    "joblib.dump(X_ecom.columns.tolist(), '../models/ecommerce_feature_names.pkl')\n",
    "print(\"Feature names saved to '../models/ecommerce_feature_names.pkl'\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"E-COMMERCE MODELING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest model: {best_ecom_model_name}\")\n",
    "print(f\"PR-AUC: {ecom_metrics_df.loc[ecom_metrics_df['Model'] == best_ecom_model_name, 'PR_AUC'].values[0]:.4f}\")\n",
    "print(f\"Recall: {ecom_metrics_df.loc[ecom_metrics_df['Model'] == best_ecom_model_name, 'Recall'].values[0]:.4f}\")\n",
    "print(f\"False Positive Rate: {ecom_metrics_df.loc[ecom_metrics_df['Model'] == best_ecom_model_name, 'False_Positive_Rate'].values[0]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
